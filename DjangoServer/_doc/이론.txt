## 가설(hypothesis)
    p-value (확률값)
    귀무 가설(null hypothesis)이 맞다는 전제 하에,
    표본에서 실제로 관측된 통계치와 '같거나 더 극단적인' 통계치가 관측될 확률이다.
## 결정트리학습(DecisionTree Learning) 에서 불순도를 계산하는 3가지 방법
    https://m.blog.naver.com/samsjang/220978650404
    지니 인덱스
    엔트로피
    분류오류
## 계단 함수(step function)
    동의어로 상수 함수(piecewise-constant function)는
    정의역을 적절한 유한 개의 구간으로 분할하였을 때 각 구간에서 상수 함수가 되는 함수이다.
## 귀무 가설(歸無假說, 영어: null hypothesis, 기호 H0) 또는 영 가설(零假說)
    통계학에서 처음부터 버릴 것을 예상하는 가설이다.
    차이가 없거나 의미있는 차이가 없는 경우의 가설이며
    이것이 맞거나 맞지 않다는 통계학적 증거를 통해 증명하려는 가설이다.
    예를 들어 범죄 사건에서 용의자가 있을 때
    형사는 이 용의자가 범죄를 저질렀다는 추정인 대립가설을 세우게 된다.
    이때 귀무가설은 용의자는 무죄라는 가설이다.
    통계적인 방법으로 가설검정(hypothesis test)을 시도할 때 쓰인다.
    로널드 피셔가 1966년에 정의하였다
## 그라운드 투루쓰 (Ground-Truth)
    https://mac-user-guide.tistory.com/14
    https://towardsdatascience.com/in-ai-the-objective-is-subjective-4614795d179b
     "label"은 정답지라고도 불리듯이 답이 명확하게 정해져 있는 값
     "ground-truth"은 '우리가 정한 정답', '우리의 모델이 우리가 원하는 답으로 예측해주길 바라는 답'
## 기댓값
    확률 변수의 기댓값(expected value)은 각 사건이 벌어졌을 때의 이득과
    그 사건이 벌어질 확률을 곱한 것을 전체 사건에 대해 합한 값이다.
    이것은 어떤 확률적 사건에 대한 평균의 의미로 생각할 수 있다. 이 경우 '모 평균'으로 다룰수있다.
## 대수의 법칙
## 대수학(algebra)
    변수는 feature 와 target 으로 나뉜다.
    상수는 계수와 편향로 나뉜다.
    따라서 다음과 같은 식의 구조를 같는다.
    target = 계수 * featureValue + 편향
    특성변수 = 독립변수 = 외생변수 = x변수
    목적변수 = 종속변수 = 내생변수 = y변수
    - 계수(係數, coefficient)는 '인자(因子)'라는 뜻으로 쓰인다.
    - 보통 식 앞에 곱해지는 상수를 말한다.
    - 가장 흔한 계수의 개념은 다항식에서 x n 앞에 붙는 수이다.
## 데이터(Data)
    -- Data has a categorical, numeric.
    카테고리(categorical) = 이산형 = norminal + ordinal = 정수형
    숫자형(numeric) = 연속형 = ratio + interval = 실수형
    -- 데이터 분석에는 두 가지의 접근방법이 있다.
    1) 확증적 데이터 분석(CDA: Confirmatory Data Analysis) = 추론통계 = 가설 -> .. -> 특정 사례 예측 = 연역
    2) 탐색적 데이터 분석(EDA: Exploratory Data Analysis) = 기술통계 = 데이터 -> .. -> 모델 = 귀납
## 데이터셋
    https://for-my-wealthy-life.tistory.com/19
    여태까지 공부를 할 때는 train set과 test set 두개로만 데이터를 나누었다.
    다만 이렇게 train, test 두개로만 분리하는 것은 기초적인 수준이고,
    보통 현업에서 모델을 만들 때는 train, test, validation set 세개로 나눈다.
    validation dataset is a sample of data held back from training your model that is used to give an estimate of model skill while tuning model’s hyperparameters.
    The validation dataset is different from the test dataset that is also held back from the training of the model, but is instead used to give an unbiased estimate of the skill of the final tuned model when comparing or selecting between final models.
    There is much confusion in applied machine learning about what a validation dataset is exactly and how it differs from a test dataset.
## 랜덤
## 로지스틱 함수
    https://datascienceschool.net/03%20machine%20learning/10.01%20%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%20%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D.html
    로지스틱 회귀분석 모형에서는 종속변수가 이항분포를 따르고 그 모수 μ가 독립변수 x에 의존한다고 가정한다.
    베르누이 확률분포를 따르는 로지스틱 회귀분석만 고려하기로 한다.
    종속변수 y가 0또는 1인 분류 예측 문제를 풀 때는 x 값을 이용하여 μ(x)를 예측한 후 다음 기준에 따라 y^값을 출력한다.
    y^={10 if μ(x)≥0.5 if μ(x)<0.5
    회귀분석을 할 때는 y^으로 y=1이 될 확률값 μ(x)를 직접 사용한다.
    y^=μ(x)
    로지스틱함수는 음의 무한대부터 양의 무한대까지의 실수값을
    0부터 1사이의 실수값으로 1 대 1 대응시키는 시그모이드함수다.
    보통 시그모이드함수라고 하면 로지스틱함수를 가리킨다.
## 로지스틱회귀
    로지스틱 회귀에는 세 가지 기본 종류가 있습니다.
        이진 로지스틱 회귀: 여기서 범주형 응답에 대해 가능한 결과는 두 가지뿐입니다.
                        위의 예에서와 같이 학생은 합격하거나 불합격합니다.
        다항 로지스틱 회귀: 여기에는 응답 변수에 순서가 없는 3개 이상의 변수가 포함될 수 있습니다.
                        예를 들어 레스토랑에서 식사하는 사람들이 특정 종류의 음식
                        (채식, 고기 또는 완전채식)을 선호하는지 예측하는 것이 있습니다.
        순서 로지스틱 회귀: 다항 회귀와 마찬가지로 3개 이상의 변수가 있을 수 있습니다.
                        그러나 측정에는 순서가 있습니다.
                        예를 들어 1에서 5까지의 척도로 호텔을 평가하는 경우를 들 수 있습니다
## 머신러닝(Machine Learning)
    Machine-Learning has a statistics, deep learning.
    The difference lies in the existence of evidence.
    머신러닝은 통계와 딥러닝의 집합이다.
    머신러닝과 딥러닝의 차이점은 신경망의 유무이다.
## 모델
    데이터를 연료 삼아 작동하는 머신 러닝 모델은 AI의 수학 엔진이자 알고리즘의 표현식으로
    인간보다 빨리 패턴을 찾고 예측을 수행합니다 -엔비디아-
    https://blogs.nvidia.co.kr/2021/08/24/what-is-a-machine-learning-model/
    모델은 선형 또는 로지스틱 회귀와 같은 다양한 유형의 알고리즘을 사용하여 인간의 전문성을 모방하는 방식으로
    데이터에서 패턴을 인식하고 결론을 도출합니다.
    간단히 말해, AI 모델링은 다음 3가지 기본 단계에 따라 의사 결정 프로세스를 생성하는 것입니다.
    https://www.intel.co.kr/content/www/kr/ko/analytics/data-modeling.html
## 분포
    이산확률변수가 따르는 확률분포인 이산확률분포 중,
    대표적인 분포인 베르누이분포(Bernoulli Distribution)와 이항분포(Binomial Distribution)에
    시행의 결과가 성공이면 1의 값을 갖고, 실패이면 0의 값을 갖는 확률변수 X를
    베르누이(Bernoulli) 확률변수라고 하고, 그 분포를 베르누이 분포라고 합니다.
    그리고 이렇게 두 가지의 결과만을 갖는 시행을 베르누이 시행이라고 합니다.
    성공일 확률은 p, 실패일 확률은 1-p = q가 되겠습니다.
    만약 베르누이 시행을 여러 번 하면 어떻게 될까요?
    예를 들어 동전 던지기도, 한번만 하고 끝내는 것이 아니라 5번을 던져서 그중 성공(앞면이라고 가정하죠)이 3번 나올 확률을 계산해볼 수도 있지 않을까요?
    이때 사용하는 확률분포를 이항분포라고 합니다.
    ㅇ 베르누이분포 : X ~ B(1,p)      (1번 만의 베르누이 시행의 성공 확률분포)
    ㅇ 이항분포     : X ~ B(n,p)      (n번 베르누이 시행의 성공 확률분포)
## 베이지안
## 불순도
    다양한 범주들의 개체들이 얼마나 포함되었는가 정도이다.
    여러가지 클래스가 섞여있는 정도이다. 반대로 순수도(purity)는 같은 클래스끼리
    얼마나 많이 포함되어 있는지를 말한다.
    https://computer-science-student.tistory.com/60
## 산술급수 와 기하급수
## 선형회귀(Linear Regression)을 코드로 구현한다.
    핵심키워드: 선형회귀(Linear Regression) 가설(Hypothesis) 비용함수(Cost function)
## 손실함수 혹은 비용함수(cost function)는 같은 용어로 통계학, 경제학 등에서 널리 쓰이는 함수로
    머신러닝에서도 손실함수는 예측값과 실제값에 대한 오차를 줄이는 데 사용된다.
    평균제곱오차 또는 음의 로그 우도함수가 있으며, 머신러닝에서도 동일한 손실함수를 사용한다.
## 시그모이드 함수
    시그모이드 함수는 S자형 곡선 또는 시그모이드 곡선을 갖는 수학 함수이다.
    시그모이드 함수의 예시로는 첫 번째 그림에 표시된 로지스틱 함수가 있다.
## 알고리즘 성능 측정지표
    https://jxo21.tistory.com/17
    https://datascienceschool.net/03%20machine%20learning/09.04%20%EB%B6%84%EB%A5%98%20%EC%84%B1%EB%8A%A5%ED%8F%89%EA%B0%80.html
    TP (True Positive) 정탐: 양성예측이 맞음 / 오류가 생겨서 보고함
    TN (True Negative) 정담: 음성예측이 맞음 / 오류가 없어서 보고없음
    FP (False Positive) 오탐: 양성예측이 틀림 / 오오류가 없는데 보고함
    FN (False Negative) 미탐: 음성예측이 틀림 / 오오류가 있는데 보고없음
    Precision 정밀도:
    Recall 재현율(=민감도): TP / (TP + FN)
    Accuracy 정확도: TP+ TN / (TP + TN + FP + FN) 맞게 예측한 샘플수의 비율-> 최적화 목적함수로 사용
    Fallout 위양성율: FP / (FP + TN)
## 엔트로피(영어: entropy, 독일어: entropie):https://ko.wikipedia.org/wiki/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC
    열역학적 계의 유용하지 않은 (일로 변환할 수 없는) 에너지의 흐름을 설명할 때 이용되는 상태 함수다.
    통계역학적으로, 주어진 거시적 상태에 대응하는 미시적 상태의 수의 로그로 생각할 수 있다.
    엔트로피는 일반적으로 보존되지 않고, 열역학 제2법칙에 따라 시간에 따라 증가한다.
    독일의 물리학자 루돌프 클라우지우스가 1850년대 초에 도입하였다.
    대개 기호로 라틴 대문자 S를 쓴다.
# 연역과 귀납
    연역은 가정된 전제이다.
    귀납은 개인적 경험이다.
## 오차 편차 잔차
    오차(Error) : 실제값(y) - 테스트 단계 예측값(y^)
    편차(Deviation) : 예측값평균 - 예측값
    잔차(Residual) :  실제값(y) - 트레인 단계 예측값(y^)
## 우도함수
    가능도 함수로 번역되기도 하고, 영어로는 likelihood function 이라 합니다)
    실현된 데이터(혹은 관찰된 데이터 observed data)로 부터
    특정 통계 모델의 적합성을 확인하는데 주로 이용됩니다.
## 인공지능의 결정적 인물들
    (1)앨런 튜링
    (2)딥러닝의 시초, 월터 피츠
    (3)인공지능의 시조, 존 매카시
    (4)딥러닝의 선구자, 로젠블랫
    (5)환상의 콤비, 사이먼과 뉴얼
    (6)인간은 생각하는 기계, 마빈 민스키
    (7)특이점이 온다, 레이 커즈와일
## 인코딩(encoding)
    문자 -> 숫자
## 정규화(Normalization)
    feature 의 변환은 표준화(Z-score 정규화)와 정규화가 있다.
    아웃라이어가 있으면 표준화 나머지는 정규화가 낫다.
## 지도학습
    지도 Learning (레이블)
    출력값(=타깃)
    int = 불연속 = 카테고리컬 = 분류 classification
    float = 연속 = 시퀀셜 = 회귀 regression
## 지도학습 분류
    classification / 회귀 regression 로 나뉜다.
    model 은 var 를 잡아내서, class 를 시도한다.
## 집계 함수
    집계 함수란 입력이 여러개의 로우이고, 출력이 하나인 결과인 것을 말합니다.
    테이블의 전체 로우 수를 구하는
    count , 평균( avg ), 총합( sum ), 최대값( max ), 최소값( min ) 등등이
    이런 함수들 입니다.
## 추론과 예측
    https://gentlej90.tistory.com/19
    https://koreapy.tistory.com/1108
    추론과 예측의 차이는 답이 라벨인가, 그라운드트루스 인가이다.
## 최적화함수
    최적화함수(Optimizer Function)는 손실함수의 결과값을 최소화하는 함수이며
    기본적으로 경사하강법(Gradient Descent)을 사용한다.
    최적화함수 또한 컴파일 과정에서 정의하며 다양한 방식들이 있다.
    경사하강법을 개선한 확률적 경사하강법(SGD: Stochastic Gradient Descent)이 있고,
    SGD를 개선한 RMSProp 방식이 있다.
## 통계와 머신러닝 차이
    기술통계 - 추론통계 = 학습(Learning)
## 편향과 편차
    https://opentutorials.org/module/3653/22071
    정답 하나를 맞추기 위해 컴퓨터는 여러 번의 예측값 내놓기를 시도하는데,
    컴퓨터가 내놓은 예측값의 동태를 묘사하는 표현이 '편향' 과 '분산' 입니다.
    예측값들과 정답이 대체로 멀리 떨어져 있으면 결과의 편향(bias)이 높다고 말하고,
    예측값들이 자기들끼리 대체로 멀리 흩어져있으면 결과의 분산(variance)이 높다고 말합니다.
    회귀 문제이든, 분류 문제이든
    첫 번째 그림과 같은 상황을 Underfitting = High Bias
    세 번째 그림과 같은 상황을 Overfitting이라고 합니다. = High Variance
## 학습(Learning)
    통계학에서 학습은 추정문제 해결과정(=추론)이다.
    learning 은 target 을 구하는 modeling 이다.
    -- 학습은 두가지 종류가 있다.
    지도학습은 샘플을 사용한다.
    비지도학습은 샘플을 사용하지 않는다.
## 확률(Probability)
    선험적 통계 = 사전, 수학적 확률, 식 -> 연역법
    경험적 통계 = 사후,  통계적 확률, 식 * "큰수의 법칙" -> 귀납법
    기대값 = 계수 * 변수 + 상수
## 확률변수(確率變數, 영어: random variable)
    확률변수는 확률 공간에서 다른 가측 공간으로 가는 가측 함수이다.
## 확률분포는 함수다
    리턴값에 따라 정수는 PMF, 실수는 PDF 를 사용한다.
    인공지능에서는 Dense 레이어를 사용하므로, 리턴값은 실수로 정의한다.
    확률분포는
    이산 - 확률질량함수 PMF: 이항분포, 다항분포, 이산균등분포, 푸아송분포, 베르누이분포, (초)기하분포
    연속 - 확률밀도함수 PDF: 정규분포(=가우스분포), 연속균등분포, 카이제곱분포, 감마분포
## 행렬 연산(Matrix Operations)
    행렬 표기법 - Matrix Notation
    행렬 덧셈 - Matrix Sum
    스칼라 곱 - Scalar Multiple
    행렬 곱 - Matrix Multiplication
    행렬의 전치 - The transpose of a matrix
## criterion 은 표준이다. 동의어로는
    standard, normal, norm, average, level 이 있다.
##  Ground-truth
    학습하고자 하는 데이터의 원본 혹은 실제 값을 표현할때 사용됩니다
    https://eair.tistory.com/16
## MSE vs. CCEE
    회귀ML 의 손실함수는 MSE 이다
    분류ML 의 손실함수는 CCEE 이다. 활성화함수로 Softmax 를 사용한다.
## OpenCV
    openCV는  BGR  사용 -> Red layer 가장 위로 -> 붉게 보임
	cv.imshow('Original', img)
	cv.waitKey(0)
	cv.destroyAllWindows()
    matplot는  RGB 사용 -> Blue layer 가장 위로 -> 푸르게 보임
	plt.imshow((lambda x: Image.fromarray(x))(img))
	plt.show()
    Blue Green Red
## transform 의 종류
    fit_transform()은 train dataset에서만 사용됩니다
    transform()은 test data에 적용하기 위해 를 사용한다.
## Z점수
    표준 점수(標準 點數, Standard score)는 통계학적으로 정규분포를 만들고
    개개의 경우가 표준편차상에 어떤 위치를 차지하는지를 보여주는 차원없는 수치이다.
    표준값, Z값(Z-value), Z 점수(Z score)이라고도 한다.