## 가설(hypothesis)
    p-value (확률값)
    귀무 가설(null hypothesis)이 맞다는 전제 하에,
    표본에서 실제로 관측된 통계치와 '같거나 더 극단적인' 통계치가 관측될 확률이다.
## 객체 검출(Object Detection)과 객체 인식(Object Recognition)
    https://bitcodic.tistory.com/94
    객체 탐지와 객체 인식은 물체 식별을 하는데 유사한 기술이다.
    그러나 이 둘은 실행함에 있어 차이점을 갖는다.
    객체 탐지는 이미지 내의 물체 존재를 찾는 과정이다.
    딥러닝의 경우, 이미지를 식별하는 것 뿐만 아니라 이미지의 위치도 찾는 객체 탐지는 객체 인식의 부분집합이다.
    이는 복수의 물체가 식별되며 같은 이미지 내에 어디에 존재하는지도 가능케한다.
    객체 인식은 정지판을 인식하거나 가로등과 보행자를 구별하는 것을 가능하게 하는 자율주행 차량 내 핵심적인 기술이다.
    이는 또한 바이오이미징과 같은 질병 확인, 산업적 분석 그리고 로봇 비젼과 같은 다양한 애플리케이션에게도 유용하다.
## 검출기
## 결정트리학습(DecisionTree Learning) 에서 불순도를 계산하는 3가지 방법
    https://m.blog.naver.com/samsjang/220978650404
    지니 인덱스
    엔트로피
    분류오류
## 계단 함수(step function)
    동의어로 상수 함수(piecewise-constant function)는
    정의역을 적절한 유한 개의 구간으로 분할하였을 때 각 구간에서 상수 함수가 되는 함수이다.
## 구조화 요소
    침식
    팽창
    열기
    닫기
## 귀무 가설(歸無假說, 영어: null hypothesis, 기호 H0) 또는 영 가설(零假說)
    통계학에서 처음부터 버릴 것을 예상하는 가설이다.
    차이가 없거나 의미있는 차이가 없는 경우의 가설이며
    이것이 맞거나 맞지 않다는 통계학적 증거를 통해 증명하려는 가설이다.
    예를 들어 범죄 사건에서 용의자가 있을 때
    형사는 이 용의자가 범죄를 저질렀다는 추정인 대립가설을 세우게 된다.
    이때 귀무가설은 용의자는 무죄라는 가설이다.
    통계적인 방법으로 가설검정(hypothesis test)을 시도할 때 쓰인다.
    로널드 피셔가 1966년에 정의하였다
## 그라운드 투루쓰 (Ground-Truth)
    https://mac-user-guide.tistory.com/14
    https://towardsdatascience.com/in-ai-the-objective-is-subjective-4614795d179b
     "label"은 정답지라고도 불리듯이 답이 명확하게 정해져 있는 값
     "ground-truth"은 '우리가 정한 정답', '우리의 모델이 우리가 원하는 답으로 예측해주길 바라는 답'
## 기댓값
    확률 변수의 기댓값(expected value)은 각 사건이 벌어졌을 때의 이득과
    그 사건이 벌어질 확률을 곱한 것을 전체 사건에 대해 합한 값이다.
    이것은 어떤 확률적 사건에 대한 평균의 의미로 생각할 수 있다. 이 경우 '모 평균'으로 다룰수있다.
## 나이키스트-새년의 표본화
## 다항회귀(Polynomial regression)
    다항 회귀는 선형 모델을 사용하여 비선형 데이터 집합을 모델링한다.
    이것은 동그란 모양의 구멍에 네모난 모양의 못 또는 말뚝을 끼워 넣는 것과 같다.
    다항 회귀는 독립 변수가 여러 개인 선형 회귀를 뜻하는 다중 선형 회귀와 비슷한 방식으로
    작동하지만, 비선형 곡선을 사용한다. 즉, 데이터 포인트가 비선형 방식으로 존재할 때 사용한다.
    모델은 이 데이터 포인트들을 지정된 수준의 다항식 특성으로 변환하고 선형 모델을 사용하여
    모델화한다. 선형 회귀에서 볼 수 있는 직선이 아닌 곡선의 다항식 선을 사용하여 최적적합을 수행한다.
    그러나 이 모델은 과대적합으로 나타나기 쉬우므로 이상한 결과치를 피하기 위해서는
    끝 부분의 곡선을 분석하는 것이 좋다.
## 대수의 법칙
## 대수학(algebra)
    변수는 feature 와 target 으로 나뉜다.
    상수는 계수와 편향로 나뉜다.
    따라서 다음과 같은 식의 구조를 같는다.
    target = 계수 * featureValue + 편향
    특성변수 = 독립변수 = 외생변수 = x변수
    목적변수 = 종속변수 = 내생변수 = y변수
    - 계수(係數, coefficient)는 '인자(因子)'라는 뜻으로 쓰인다.
    - 보통 식 앞에 곱해지는 상수를 말한다.
    - 가장 흔한 계수의 개념은 다항식에서 x n 앞에 붙는 수이다.
## 덕 타이핑(duck typing)
    동적 타이핑의 한 종류로, 객체의 변수 및 메소드의 집합이 객체의 타입을 결정하는 것을 말한다.
    클래스 상속이나 인터페이스 구현으로 타입을 구분하는 대신,
    덕 타이핑은 객체가 어떤 타입에 걸맞은 변수와 메소드를 지니면
    객체를 해당 타입에 속하는 것으로 간주한다.
    “덕 타이핑”이라는 용어는 다음과 같이 표현될 수 있는 덕 테스트에서 유래했다.
    (덕은 영어로 오리를 의미한다.)
    사람이 오리처럼 행동하면 오리로 봐도 무방하다라는게 덕 타이핑(Duck Typing)이다.
    타입을 미리 정하는게 아니라 실행이 되었을 때 해당 Method들을 확인하여 타입을 정한다.
    장점
        타입에 대해 매우 자유롭다.
        런타임 데이터를 기반으로 한 기능과 자료형을 창출하는 것
    단점
        런타임 자료형 오류가 발생할 수 있다 런타임에서, 값은 예상치 못한 유형이 있을 수 있고,
        그 자료형에 대한 무의미한 작업이 적용된다.
        이런 오류가 프로그래밍 실수 구문에서 오랜 시간 후에 발생할 수 있다
        데이터의 잘못된 자료형의 장소로 전달되는 구문은 작성하지 않아야 한다.
        이것은 버그를 찾기 어려울 수도 있다.
## 데이터(Data)
    -- Data has a categorical, numeric.
    카테고리(categorical) = 이산형 = norminal + ordinal = 정수형
    숫자형(numeric) = 연속형 = ratio + interval = 실수형
    -- 데이터 분석에는 두 가지의 접근방법이 있다.
    1) 확증적 데이터 분석(CDA: Confirmatory Data Analysis) = 추론통계 = 가설 -> .. -> 특정 사례 예측 = 연역
    2) 탐색적 데이터 분석(EDA: Exploratory Data Analysis) = 기술통계 = 데이터 -> .. -> 모델 = 귀납
## 데이터셋
    https://for-my-wealthy-life.tistory.com/19
    여태까지 공부를 할 때는 train set과 test set 두개로만 데이터를 나누었다.
    다만 이렇게 train, test 두개로만 분리하는 것은 기초적인 수준이고,
    보통 현업에서 모델을 만들 때는 train, test, validation set 세개로 나눈다.
    validation dataset is a sample of data held back from training your model that is used to give an estimate of model skill while tuning model’s hyperparameters.
    The validation dataset is different from the test dataset that is also held back from the training of the model, but is instead used to give an unbiased estimate of the skill of the final tuned model when comparing or selecting between final models.
    There is much confusion in applied machine learning about what a validation dataset is exactly and how it differs from a test dataset.
## 라쏘 회귀(Lasso regression)
    라쏘 회귀는 리지 회귀와 같이 모델의 복잡성을 줄여주는 또 다른 정규화 기법이다.
    회귀 계수의 절대 사이즈를 금지함으로써 복잡성을 줄인다.
    리지 회귀와는 다르게 아예 계수 값을 0에 가깝게 만든다.

그 장점은 기능 선택을 사용할 수 있다는 것이다. 데이터 집합에서 기능 세트를 선택하여 모델을 구축할 수 있다. 라쏘 회귀는 필요한 요소들만 사용하고 나머지를 0으로 설정함으로써 과대적합을 방지할 수 있다.
## 랜덤
## 로지스틱 함수
    https://datascienceschool.net/03%20machine%20learning/10.01%20%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%20%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D.html
    로지스틱 회귀분석 모형에서는 종속변수가 이항분포를 따르고 그 모수 μ가 독립변수 x에 의존한다고 가정한다.
    베르누이 확률분포를 따르는 로지스틱 회귀분석만 고려하기로 한다.
    종속변수 y가 0또는 1인 분류 예측 문제를 풀 때는 x 값을 이용하여 μ(x)를 예측한 후 다음 기준에 따라 y^값을 출력한다.
    y^={10 if μ(x)≥0.5 if μ(x)<0.5
    회귀분석을 할 때는 y^으로 y=1이 될 확률값 μ(x)를 직접 사용한다.
    y^=μ(x)
    로지스틱함수는 음의 무한대부터 양의 무한대까지의 실수값을
    0부터 1사이의 실수값으로 1 대 1 대응시키는 시그모이드함수다.
    보통 시그모이드함수라고 하면 로지스틱함수를 가리킨다.
## 로지스틱회귀
    종속 변수에 이산 값이 있는 경우, 다시 말해 0 또는 1, 참 또는 거짓, 흑 또는 백, 스팸 또는 스팸 아닌 것 등의
    두 가지 값 중 하나만 취할 수 있는 경우 로지스틱 회귀를 사용하여 데이터를 분석할 수 있다.
    로지스틱 회귀에는 세 가지 기본 종류가 있습니다.
        이진 로지스틱 회귀: 여기서 범주형 응답에 대해 가능한 결과는 두 가지뿐입니다.
                        위의 예에서와 같이 학생은 합격하거나 불합격합니다.
        다항 로지스틱 회귀: 여기에는 응답 변수에 순서가 없는 3개 이상의 변수가 포함될 수 있습니다.
                        예를 들어 레스토랑에서 식사하는 사람들이 특정 종류의 음식
                        (채식, 고기 또는 완전채식)을 선호하는지 예측하는 것이 있습니다.
        순서 로지스틱 회귀: 다항 회귀와 마찬가지로 3개 이상의 변수가 있을 수 있습니다.
                        그러나 측정에는 순서가 있습니다.
                        예를 들어 1에서 5까지의 척도로 호텔을 평가하는 경우를 들 수 있습니다
## 리지 회귀(Ridge regression)
    Ridge: 능선(稜線)은 산이나 언덕의 꼭대기가 일정 간격을 두고 연결되어 연속적으로 솟아오른 지형을 말한다. 등산 코스로 흔히 사용된다
    다중 회귀라고도 불리는 리지 회귀는 정규화 또는 규제화(regularization) 기법으로 알려져 있으며
    모델의 복잡성을 줄이는 데 사용된다.
    또한 ‘리지 회귀 페널티’로 알려진 약간의 편향, 즉 바이어스(bias)를 사용하여
    모델이 과대적합(overfitting)에 덜 취약하게 만든다.
## 마르-힐드레스 엣지 검출기
## 마하밴드
## 머신러닝(Machine Learning)
    Machine-Learning has a statistics, deep learning.
    The difference lies in the existence of evidence.
    머신러닝은 통계와 딥러닝의 집합이다.
    머신러닝과 딥러닝의 차이점은 신경망의 유무이다.
## 모델
    데이터를 연료 삼아 작동하는 머신 러닝 모델은 AI의 수학 엔진이자 알고리즘의 표현식으로
    인간보다 빨리 패턴을 찾고 예측을 수행합니다 -엔비디아-
    https://blogs.nvidia.co.kr/2021/08/24/what-is-a-machine-learning-model/
    모델은 선형 또는 로지스틱 회귀와 같은 다양한 유형의 알고리즘을 사용하여 인간의 전문성을 모방하는 방식으로
    데이터에서 패턴을 인식하고 결론을 도출합니다.
    간단히 말해, AI 모델링은 다음 3가지 기본 단계에 따라 의사 결정 프로세스를 생성하는 것입니다.
    https://www.intel.co.kr/content/www/kr/ko/analytics/data-modeling.html
## 밀집 데이터(Dense Data)
    dense data는 차원/전체 공간에 비해 데이터가 있는 공간이 빽빽하게 차 있는 데이터이다.
## 믹스인(mixin)
    객체 지향 프로그래밍 언어에서 믹스인(mixin)은 다른 클래스의 부모클래스가 되지 않으면서
    다른 클래스에서 사용할 수 있는 메서드를 포함하는 클래스이다.
    다른 클래스가 믹스인의 메소드에 액세스하는 방법은 언어에 따라 다르다.
    믹스인은 때때로 "상속"이 아니라 "포함"으로 설명된다.
    믹스인은 코드재사용성을 높이고 다중상속으로 인해 발생할 수 있는
    상속의 모호성 문제("다이아몬드 문제")를 제거하거나
    언어에서 다중상속에 대한 지원부족을 해결하기 위해 사용될 수 있다.
    믹스인은 구현된 메서드가 포함된 인터페이스로 볼 수도 있다.
    이 패턴은 종속성 역전 원칙을 적용하는 예가 되기도 한다.
## 베이지안
## 분류 와 회귀
    참조: https://nexablue.tistory.com/29
    회귀는 확률을 예측하는 것이 아니다. 회귀는 출력는 연속성이 있고, 그 연속성 중에 어디에 점을 찍을지 결정하는 문제이다.
## 분포
    이산확률변수가 따르는 확률분포인 이산확률분포 중,
    대표적인 분포인 베르누이분포(Bernoulli Distribution)와 이항분포(Binomial Distribution)에
    시행의 결과가 성공이면 1의 값을 갖고, 실패이면 0의 값을 갖는 확률변수 X를
    베르누이(Bernoulli) 확률변수라고 하고, 그 분포를 베르누이 분포라고 합니다.
    그리고 이렇게 두 가지의 결과만을 갖는 시행을 베르누이 시행이라고 합니다.
    성공일 확률은 p, 실패일 확률은 1-p = q가 되겠습니다.
    만약 베르누이 시행을 여러 번 하면 어떻게 될까요?
    예를 들어 동전 던지기도, 한번만 하고 끝내는 것이 아니라 5번을 던져서 그중 성공(앞면이라고 가정하죠)이 3번 나올 확률을 계산해볼 수도 있지 않을까요?
    이때 사용하는 확률분포를 이항분포라고 합니다.
    ㅇ 베르누이분포 : X ~ B(1,p)      (1번 만의 베르누이 시행의 성공 확률분포)
    ㅇ 이항분포     : X ~ B(n,p)      (n번 베르누이 시행의 성공 확률분포)

## 불순도
    다양한 범주들의 개체들이 얼마나 포함되었는가 정도이다.
    여러가지 클래스가 섞여있는 정도이다. 반대로 순수도(purity)는 같은 클래스끼리
    얼마나 많이 포함되어 있는지를 말한다.
    https://computer-science-student.tistory.com/60
## 산술급수 와 기하급수
## 선형 회귀(Linear regression)
    예측 변수와 종속 변수로 구성되며, 이 둘은 선형 방식으로 서로 연관지어져 있다.
    선형 회귀는 위에서 설명한 대로 가장 적합한 선, 즉 최적적합선을 사용한다.
    변수들이 서로 선형적으로 연결되어 있는 경우 선형 회귀를 사용한다.
    광고 지출 증가가 판매에 미치는 영향을 예측할 때 등이 예가 될 수 있다.
    그러나 선형 회귀분석은 특이치에 영향을 받기 쉬우므로 빅데이터 집합을 분석하는 데 사용해서는 안 된다.
    핵심키워드: 선형회귀(Linear Regression) 가설(Hypothesis) 비용함수(Cost function)
## 셀레니움(Selenium)

## 소벨 엣지 연산자
## 손실함수 혹은 비용함수(cost function)는 같은 용어로 통계학, 경제학 등에서 널리 쓰이는 함수로
    머신러닝에서도 손실함수는 예측값과 실제값에 대한 오차를 줄이는 데 사용된다.
    평균제곱오차 또는 음의 로그 우도함수가 있으며, 머신러닝에서도 동일한 손실함수를 사용한다.
## 수학적 형태학
    적중-비적중 변환
    세션화
## 스웨거(Swagger)
    개발자가 REST 웹 서비스를 설계, 빌드, 문서화, 소비하는 일을 도와주는
    대형 도구 생태계의 지원을 받는 오픈 소스 소프트웨어 프레임워크이다.
## 시그모이드 함수
    시그모이드 함수는 S자형 곡선 또는 시그모이드 곡선을 갖는 수학 함수이다.
    시그모이드 함수의 예시로는 첫 번째 그림에 표시된 로지스틱 함수가 있다.
## 시리얼라이즈(Serialize)
## 시리즈(Series)
## 시퀀스(Sequence)
## 시퀀셜(Sequential)


## 시계열 데이터
    : 일련의 순차적으로 정해진 데이터 셋의 집합
    : 시간에 관해 순서가 매겨져 있다는 점과, 연속한 관측치는 서로 상관관계를 갖고 있다
    회귀분석
    : 관찰된 연속형 변수들에 대해 두 변수 사이의 모형을 구한뒤 적합도를 측정해 내는 분석 방법
## 알고리즘 성능 측정지표
    https://jxo21.tistory.com/17
    https://datascienceschool.net/03%20machine%20learning/09.04%20%EB%B6%84%EB%A5%98%20%EC%84%B1%EB%8A%A5%ED%8F%89%EA%B0%80.html
    TP (True Positive) 정탐: 양성예측이 맞음 / 오류가 생겨서 보고함
    TN (True Negative) 정담: 음성예측이 맞음 / 오류가 없어서 보고없음
    FP (False Positive) 오탐: 양성예측이 틀림 / 오오류가 없는데 보고함
    FN (False Negative) 미탐: 음성예측이 틀림 / 오오류가 있는데 보고없음
    Precision 정밀도:
    Recall 재현율(=민감도): TP / (TP + FN)
    Accuracy 정확도: TP+ TN / (TP + TN + FP + FN) 맞게 예측한 샘플수의 비율-> 최적화 목적함수로 사용
    Fallout 위양성율: FP / (FP + TN)
## 엔트로피(영어: entropy, 독일어: entropie):https://ko.wikipedia.org/wiki/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC
    열역학적 계의 유용하지 않은 (일로 변환할 수 없는) 에너지의 흐름을 설명할 때 이용되는 상태 함수다.
    통계역학적으로, 주어진 거시적 상태에 대응하는 미시적 상태의 수의 로그로 생각할 수 있다.
    엔트로피는 일반적으로 보존되지 않고, 열역학 제2법칙에 따라 시간에 따라 증가한다.
    독일의 물리학자 루돌프 클라우지우스가 1850년대 초에 도입하였다.
    대개 기호로 라틴 대문자 S를 쓴다.
## 연역과 귀납
    연역은 가정된 전제이다.
    귀납은 개인적 경험이다.
## 영상처리
    HoG(Histogram of Oriented Gradient) 서술자
    blob(Binary Large Object)
    LoG(Laplacian of Gaussian)
    DoG(Difference of Gaussians)
    SIFT(Scale-Invariant Feature Transform)
    동차좌표
    k-d 트리
    RANSAC(RANdom SAmple Consensus)
## 오차 편차 잔차
    오차(Error) : 실제값(y) - 테스트 단계 예측값(y^)
    편차(Deviation) : 예측값평균 - 예측값
    잔차(Residual) :  실제값(y) - 트레인 단계 예측값(y^)
## 우도함수
    가능도 함수로 번역되기도 하고, 영어로는 likelihood function 이라 합니다)
    실현된 데이터(혹은 관찰된 데이터 observed data)로 부터
    특정 통계 모델의 적합성을 확인하는데 주로 이용됩니다.
## 워드임베딩(Word Embedding)
    인간의 언어(자연어)는 수치화되어 있지 않은 데이터이기 때문에 머신러닝, 딥러닝 기법을
    바로 사용할 수가 없다. 수치화되어있는 데이터의 예로는 Mnist나 꽃의 종류처럼
    숫자로 분류가 가능한 것들을 말한다. 그래서 자연어 처리에서 특징 추출을 통해
    수치화를 해줘야 하는데 이때 사용하는 것이 "언어의 벡터화"이다.
    이런 벡터화의 과정을 Word Embedding이라고 한다.
    https://simpling.tistory.com/entry/Embedding-%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0
## 원핫인코딩 (One Hot Encoding)
    가장 기본적인 벡터화의 방법은 One-hot encoding 방법이다.
    예를 들어, 남자와 여자를 표현하는 벡터를 만든다고 할 때 각각을 [1,0] [0,1]로 만드는 방법이다.
    그런데 이 방법은 단어가 많아지면 벡터 공간이 매우 커지고 실제 1인 값은 한 개뿐 이므로 매우 비효율적이다.
    또 이런 표현방식은 단어가 뭔지만을 알려줄 뿐 어떤 특징을 표출해주지는 못한다.
    이런 방식을 단어 벡터의 크기가 너무 크고 값이 1이 되는 값은 거의 없어
    Sparse(희소)한 표현법이라고 한다.
## 이미지 읽기의 flag는 3가지가 있습니다.
    cv2.IMREAD_COLOR : 이미지 파일을 Color로 읽어들입니다.
                        투명한 부분은 무시되며, Default값입니다.
    cv2.IMREAD_GRAYSCALE : 이미지를 Grayscale로 읽어 들입니다.
                            실제 이미지 처리시 중간단계로 많이 사용합니다.
    cv2.IMREAD_UNCHANGED : 이미지파일을 alpha channel까지 포함하여 읽어 들입니다
    3개의 flag대신에 1, 0, -1을 사용해도 됩니다
    Shape is (512, 512, 3)
    Y축: 512 (앞)
    X축: 512 (뒤)
    3은 RGB 로 되어 있다.
    cv2.waitKey(0) : keyboard입력을 대기하는 함수로
                    0이면 key입력까지 무한대기이며, 특정 시간동안 대기하려면
                    milisecond값을 넣어주면 됩니다.
    cv2.destroyAllWindows() 화면에 나타난 윈도우를 종료합니다.
                            일반적으로 위 3개는 같이 사용됩니다.
## 이진화
    전역 임계치 결정
    적응적 임계치 결정
## 인공지능의 결정적 인물들
    (1)앨런 튜링
    (2)딥러닝의 시초, 월터 피츠
    (3)인공지능의 시조, 존 매카시
    (4)딥러닝의 선구자, 로젠블랫
    (5)환상의 콤비, 사이먼과 뉴얼
    (6)인간은 생각하는 기계, 마빈 민스키
    (7)특이점이 온다, 레이 커즈와일
## 인코딩(encoding)
    문자 -> 숫자
## 잡음(Noise)
    가우시안 잡음(Gaussian Noise)
    점 잡음(Salt-And-Pepper Noise)
## 점확산함수(Point Spread Function)
## 정규화(Normalization)
    feature 의 변환은 표준화(Z-score 정규화)와 정규화가 있다.
    아웃라이어가 있으면 표준화 나머지는 정규화가 낫다.
## 지도학습
    지도 Learning (레이블)
    출력값(=타깃)
    int = 불연속 = 카테고리컬 = 분류 classification
    float = 연속 = 시퀀셜 = 회귀 regression
## 지도학습 분류
    classification / 회귀 regression 로 나뉜다.
    model 은 var 를 잡아내서, class 를 시도한다.
## 직렬화(Serialization)


## 집계 함수
    집계 함수란 입력이 여러개의 로우이고, 출력이 하나인 결과인 것을 말합니다.
    테이블의 전체 로우 수를 구하는
    count , 평균( avg ), 총합( sum ), 최대값( max ), 최소값( min ) 등등이
    이런 함수들 입니다.

## 추론과 예측
    https://gentlej90.tistory.com/19
    https://koreapy.tistory.com/1108
    추론과 예측의 차이는 답이 라벨인가, 그라운드트루스 인가이다.
## 최적화함수
    최적화함수(Optimizer Function)는 손실함수의 결과값을 최소화하는 함수이며,
    기본적으로 경사하강법(Gradient Descent)을 사용한다.
    최적화함수 또한 컴파일 과정에서 정의하며 다양한 방식들이 있다.
    경사하강법을 개선한 확률적 경사하강법(SGD: Stochastic Gradient Descent)이 있고,
    SGD를 개선한 RMSProp 방식이 있다.
## 케라스
    케라스(Keras)는 파이썬으로 작성된 오픈 소스 신경망 라이브러리이다
## 코퍼스(Corpus)
    말뭉치 또는 코퍼스(Corpus)는 자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합이다.
    RNN에서 TrainSet을 말한다. 코퍼스에서 소문자(lower_case)전환, 숫자제거, 문장기호를 제거한
    상태를 토큰이라고 한다. 토큰은 정수화된다. 원핫인코딩으로 정형화된다.
    어휘사전은 코퍼스에서 고유한 단어를 뽑아 만든 목록이다. 어휘사전에 없는 단어는 2로 변환한다.
    1은 문장의 시작이다. num_words 는 빈도수가 높은 상위 단어이다.
    정형화가 된 훈련데이터셋은 파이썬 리스트 객체로 이루어진 넘파이 배열이다.
    해결할 문제는 긍정과 부정 판단이다. 타깃값이 0(부정), 1(긍정)으로 나누어진다.
## 타깃
    출력값(=타깃)
     - int = 불연속 = 카테고리컬 = 분류 classification
     - float = 연속 = 시퀀셜 = 회귀 regression
## 텐서플로
    TensorFlow는 머신 러닝을 위한 엔드 투 엔드 오픈 소스 플랫폼입니다.
## 토큰
    코퍼스에서 숫자와 기호가 제거된 사전에 실린 단어 그 자체를 토큰이라고 한다.
    토큰(Token)이란 문법적으로 더 이상 나눌 수 없는 언어요소를 뜻합니다.
    텍스트 토큰화의 유형은 문장 토큰화와 단어 토큰화로 나눌 수 있습니다.
    문장 토큰화는 텍스트에서 문장을 분리하는 작업을 뜻한다.
    단어 토큰화는 문자아에서 단어를 토큰으로 분리하는 작업을 뜻합니다.
    단어 토큰화 (Word Tokenization)는 기본적으로 띄어쓰기를 기준으로 합니다.


## 토크나이저
    Tokenize는 data를 문장 혹은 단어 등의 단위로 나누는 것을 말한다
## 통계와 머신러닝 차이
    기술통계 - 추론통계 = 학습(Learning)
## 특성맵
    합성곱에서 리턴되는 함수이다.
## 파이토치
    토치(Torch) 및 카페2(Caffe2)를 기반으로 한 텐서플로와 유사한 딥러닝 라이브러리이다.
    페이스북 인공지능 연구팀에 의해 주로 개발되어 왔다.
    텐서플로 2.0 이후의 친 keras 행보로 자유로운 네트워크 수정의 난이도가 점점 높아져,
    최근 연구원들 사이에서는 PyTorch의 사용 비중이 높아지고 있다.
    흔히 파이소닉(Pythonic)한 코드를 사용한다고 알려져 있다. - 출처: 나무위키 -
## 편향과 편차
    https://opentutorials.org/module/3653/22071
    정답 하나를 맞추기 위해 컴퓨터는 여러 번의 예측값 내놓기를 시도하는데,
    컴퓨터가 내놓은 예측값의 동태를 묘사하는 표현이 '편향' 과 '분산' 입니다.
    예측값들과 정답이 대체로 멀리 떨어져 있으면 결과의 편향(bias)이 높다고 말하고,
    예측값들이 자기들끼리 대체로 멀리 흩어져있으면 결과의 분산(variance)이 높다고 말합니다.
    회귀 문제이든, 분류 문제이든
    첫 번째 그림과 같은 상황을 Underfitting = High Bias
    세 번째 그림과 같은 상황을 Overfitting이라고 합니다. = High Variance
## 패팅(Padding)
    합성곱에서 입력 행렬의 주의를 가상의 원소(0값)으로 채우는 것이다.
    종류는 세임패딩(Same Padding)과 밸리드패딩(Valid Padding)이 있다.
## 프리윗 엣지 연산자
## 필터(Filter)
    OpenCV에서 필터는 다음과 같다
        저역통과 필터
        쌍방 필터
        중간값 필터
    합성곱에서 필터는 뉴런의 행렬구조로 된 집합이다.

## 캐니 엣지 검출기
    1. 스무딩(블러): 가우시안 필터를 이용한 노이즈 제거한다
    2. 그레디언트: 소벨필터를 이용한 그래디언트의 크기(intensity)를 찾는다
    3. Non-maximum suppression을 적용하여 거짓 반응을 제거한다.
    4. 경계선으로써 가능성 있는 픽셀을 골라내기 위해 double threshold 방식을 적용한다.
    5. 앞서 double threshold 방식에서 maxVal을 넘은 부분을 strong edge,
    minVal과 maxVal 사이의 부분을 weak edge로 설정하여
    strong edge와 연결되어 있는 weak edge를 edge로 판단하고 그렇지 않는 부분은 제거한다.
    (Hysteresis thresholding)
## 커널
    합성곱에서 커널은 가중치이다.
## 학습(Learning)
    통계학에서 학습은 추정문제 해결과정(=추론)이다.
    learning 은 target 을 구하는 modeling 이다.
    -- 학습은 두가지 종류가 있다.
    지도학습은 샘플을 사용한다.
    비지도학습은 샘플을 사용하지 않는다.
## 함수형 프로그래밍은 속성을 제거한 프로그래밍 패러다임이다.
    함수형 프로그래밍 언어로 설계된 클로저,스칼라,하스켈 등의 언어가 있고,
    자바, 자바스크립트,코틀린,파이썬 등에도
    최근 버전에 함수형 프로그래밍 문법이 추가 되었다.
    함수형 프로그래밍은 객체 지향형 프로그래밍과는 다른 새로운 방식이다.
    새로운 계산방법을 배우는 것 처럼 사고의 전환을 필요로 하며
    다양한 사고방식으로 프로그래밍을 바라보면 유연한 문제해결이 가능하다.
## 합성곱(convolution)
    합성곱은 함수(=입력)와 함수(=필터)가 커널(가중치)을 곱하여 새로운 함수(=피처맵)를 리턴하는 연산자이다.
## 확률(Probability)
    선험적 통계 = 사전, 수학적 확률, 식 -> 연역법
    경험적 통계 = 사후,  통계적 확률, 식 * "큰수의 법칙" -> 귀납법
    기대값 = 계수 * 변수 + 상수
## 확률변수(確率變數, 영어: random variable)
    확률변수는 확률 공간에서 다른 가측 공간으로 가는 가측 함수이다.
## 확률분포는 함수다
    리턴값에 따라 정수는 PMF, 실수는 PDF 를 사용한다.
    인공지능에서는 Dense 레이어를 사용하므로, 리턴값은 실수로 정의한다.
    확률분포는
    이산 - 확률질량함수 PMF: 이항분포, 다항분포, 이산균등분포, 푸아송분포, 베르누이분포, (초)기하분포
    연속 - 확률밀도함수 PDF: 정규분포(=가우스분포), 연속균등분포, 카이제곱분포, 감마분포
## 행렬 연산(Matrix Operations)
    행렬 표기법 - Matrix Notation
    행렬 덧셈 - Matrix Sum
    스칼라 곱 - Scalar Multiple
    행렬 곱 - Matrix Multiplication
    행렬의 전치 - The transpose of a matrix
## 효과(Effect)
    동시대비효과
    표본화(Sampling)
    양자화(Quantization)
    평활화(Smoothing)
## 휴리스틱 이론
    불충분한 시간이나 데이터로 인해 합리적인 판단이 불가능하거나
    굳이 필요하지 않은 상태에서 빠르게 사용하는 어림짐작
## 활성화 함수
    https://tykimos.github.io/2017/01/27/MLP_Layer_Talk/
    Dense(8, input_dim=4, init='uniform', activation='relu'))
    주요 인자는 다음과 같습니다.
    첫번째 인자 : 출력 뉴런의 수를 설정합니다.
    input_dim : 입력 뉴런의 수를 설정합니다.
    init : 가중치 초기화 방법 설정합니다.
    ‘uniform’ : 균일 분포
    ‘normal’ : 가우시안 분포
    activation : 활성화 함수 설정합니다.
    ‘linear’ : 디폴트 값, 입력뉴런과 가중치로 계산된 결과값이 그대로 출력으로 나옵니다.
    ‘relu’ : rectifier 함수, 은익층에 주로 쓰입니다.
    ‘sigmoid’ : 시그모이드 함수, 이진 분류 문제에서 출력층에 주로 쓰입니다.
    ‘softmax’ : 소프트맥스 함수, 다중 클래스 분류 문제에서 출력층에 주로 쓰입니다.
## 회귀모델

    회귀모델을 평가하는 평가지표
    회귀모델은 그 모델이 잘 학습되어졌는지 확인하기 위한 회귀모델의 평가지표들이 4가지 있다. 이를 하나씩 살펴보자.
## 회귀분석의 다섯가지 타입
    1. 선형 회귀(Linear regression)
    2. 로지스틱 회귀(Logistic regression)
    3. 리지 회귀(Ridge regression)
    4. 라쏘 회귀(Lasso regression)
    5. 다항 회귀(Polynomial regression)
    https://www.appier.com/ko-kr/blog/5-types-of-regression-analysis-and-when-to-use-them
## 희소 데이터(sparse data)
    saprse data는 차원/전체 공간에 비해 데이터가 있는 공간이 매우 협소한 데이터이다.
    행렬 중에 희소 행렬(sparse matrix)은 행렬의 값 대부분이 0을 나타낸다.
    머신 러닝에서는 매우 고차원인 데이터를 다루는 경우가 많다.
    데이터가 sparse 해질 확률도 높다는 의미이다.
    이 sparse data를 가지고 learning을 할시 잘 되지 않는다.
    그렇기에 사전에 NLP에서 사용하는 word/sentence embedding 등과 같이
    차원 축소(dimension reduction) 과정을 통해
    데이터를 dense 하게 만들 필요성이 존재하는 것이다.
## Bert **프로젝트 사용 라이브러리
## CBow
    CBow의 예를 들면, 나는 오늘 __ 를 갔다. 의 문장에서 __를 예측하는 것이다. 학습은 다음과 같이 진행된다.
    1. 주변 단어들을 one-hot 벡터로 만들어 입력값으로 사용한다.(input layer)
    2. 가중치 행렬을 one-hot벡터에 곱해서 n-차원 벡터를 만든다.(hidden layer)
    3. 만들어진 n-차원 벡터를 모두 더해 더한 개수로 나눠 평균 n-차원 벡터를 만든다.(output layer)
    4. n-차원 벡터에 다시 가중치 행렬을 곱해서 one-hot 벡터와 같은 차원의 벡터로 만들고 실제 예측하려고 하는 단어의 one-hot 벡터와 비교해서 학습한다.(optimize)
    위의 학습 과정을 모두 끝낸 후 가중치 행렬의 각 행을 단어 벡터로 사용한다.
    이런 과정을 거쳐서 만들어진 단어 벡터는 단어 간의 유사도를 잘 측정하며 복잡한 특징까지도 잘 잡아낸다.
    예를 들어 엄마 아빠, 여자 남자라는 단어의 벡터 사이의 거리가 같게 나오는 것이다.
## CLIP **프로젝트 사용 라이브러리
## CountVectorizer
    단순히 각 텍스트에서 횟수를 기준으로 특징을 추출하는 방법이다.
    예를 들어 '나는 매일매일 일기를 쓴다'를 벡터화할 때 단어 사전을 먼저 정의해야 하는데
    단어 사전이 [나는, 매일 , 혼자, 일기를, 쓴다] 일 때, [1, 2, 0, 1, 1]로 되는 것이다.
    이 방법은 단순히 횟수만을 특징으로 잡기 때문에 큰 의미가 없고
    자주 쓰이는 조사가 높은 특징 값을 가진다는 단점이 있다.
    이를 해결하기 위해 사용하는 것이 TfidfVectorizer이다.
## CRAFT **프로젝트 사용 라이브러리
## criterion 은 표준이다. 동의어로는
    standard, normal, norm, average, level 이 있다.
## CRNN **프로젝트 사용 라이브러리
## DeepFaceLab(DFL) **프로젝트 사용 라이브러리

## FaceSwap **프로젝트 사용 라이브러리

## FFMPEG **프로젝트 사용 라이브러리
## Ground-truth
    학습하고자 하는 데이터의 원본 혹은 실제 값을 표현할때 사용됩니다
    https://eair.tistory.com/16
## Image Captioning

## K-평균 알고리즘
    군집화(Cluster) 문제를 풀기위한 비지도 학습 알고리즘.
    주어진 데이터를 군집갯수(K) 로 그룹화하여 동일한 성질을 가지고
    다른 그룹과 차별화 시키는 것.
    알고리즘의 결과는 중심(centroid) 라고 부르는 K개의 점(dot)으로,
    이름은 각기 다른 그룹의 중심점을 나타낸다.
    k-평균 클러스터링 알고리즘은 클러스터링 방법 중 분할법에 속한다.
    분할법은 주어진 데이터를 여러 파티션 (그룹) 으로 나누는 방법이다.
    예를 들어 n개의 데이터 오브젝트를 입력받았다고 가정하자.
    이 때 분할법은 입력 데이터를 n보다 작거나 같은 k개의 그룹으로 나누는데,
    이 때 각 군집은 클러스터를 형성하게 된다.
    다시 말해, 데이터를 한 개 이상의 데이터 오브젝트로 구성된 k개의 그룹으로 나누는 것이다.
    이 때 그룹을 나누는 과정은 거리 기반의 그룹간
    비유사도 (dissimilarity) 와 같은 비용 함수 (cost function) 을
    최소화하는 방식으로 이루어지며,
    이 과정에서 같은 그룹 내 데이터 오브젝트 끼리의 유사도는 증가하고,
    다른 그룹에 있는 데이터 오브젝트와의 유사도는 감소하게 된다.
    k-평균 알고리즘은 각 그룹의 중심 (centroid)과
    그룹 내의 데이터 오브젝트와의 거리의 제곱합을 비용 함수로 정하고,
    이 함수값을 최소화하는 방향으로 각 데이터 오브젝트의 소속 그룹을
    업데이트 해 줌으로써 클러스터링을 수행하게 된다.
## KoGPT2 **프로젝트 사용 라이브러리

## loc vs iloc
    DF에서 loc는 키 값으로 검색 하고 iloc는 인덱스 값으로 검색한다.
## MAE (평균절대오차:Mean Absolute Error)
    회귀평가를 위한 지표로 주로 쓰인다.
    기계 학습 모델의 퀄리티를 요약하고 평가하기 위한 여러 메트릭 중 하나라고 할 수 있다.
    MSE와 마찬가지로 0에 가까울수록 좋은 모델이라고 할 수 있다.

## MSE vs. CCEE
    평균제곱오차 (MSE, Mean Squared Error): 추측값에 대한 정확성을 측정하는 방법이다.
    머신러닝에서는 Cost Function(손실함수 혹은 비용함수)에서,
    영상처리에서는 화질 개선을 위해 원본 대비 화질을 측정하는 PSNR에서 주로 쓰인다.
    회귀ML 의 손실함수는 MSE 이다
    분류ML 의 손실함수는 CCEE 이다. 활성화함수로 Softmax 를 사용한다.
## NLTK
    자연어 처리를 위한 파이썬 패키지입니다.
    아나콘다를 설치하였다면 NLTK는 기본적으로 설치가 되어져 있습니다.

## NPY 파일
    NumPy 라이브러리가 설치된 Python 소프트웨어 패키지에 의해 만들어진
    NumPy 배열 파일입니다.
    np.save ( 'filename.npy', array)를 사용하여 NPY 파일로 배열을 내보낼 수 있습니다.
    np를 사용하여 NPY 파일에 배열을로드 할 수 있습니다 .load ( 'filename.npy').

## OpenCV
    openCV는  BGR  사용 -> Red layer 가장 위로 -> 붉게 보임
	cv.imshow('Original', img)
	cv.waitKey(0)
	cv.destroyAllWindows()
    matplot는  RGB 사용 -> Blue layer 가장 위로 -> 푸르게 보임
	plt.imshow((lambda x: Image.fromarray(x))(img))
	plt.show()
## PSNR (최대 신호 대 잡음비(Peak Signal-to-noise ratio)
    신호가 가질 수 있는 최대 전력에 대한 잡음의 전력을 나타낸 것이다.
    주로 영상 또는 동영상 손실 압축에서 화질 손실 정보를 평가할 때 사용된다.
    최대 신호 대 잡음비는 신호의 전력에 대한 고려 없이
    평균 제곱 오차(MSE)를 이용해서 계산 할 수 있다.
## REST API 보안
    https://docs.iamport.kr/tech/access-token
    '''
    iss: 토큰 발급자 (issuer)
    sub: 토큰 제목 (subject)
    aud: 토큰 대상자 (audience)
    exp: 토큰의 만료시간 (expiraton), 시간은 NumericDate 형식으로 되어있어야 하며 (예: 1480849147370) 언제나 현재 시간보다 이후로 설정되어있어야합니다.
    nbf: Not Before 를 의미하며, 토큰의 활성 날짜와 비슷한 개념입니다. 여기에도 NumericDate 형식으로 날짜를 지정하며, 이 날짜가 지나기 전까지는 토큰이 처리되지 않습니다.
    iat: 토큰이 발급된 시간 (issued at), 이 값을 사용하여 토큰의 age 가 얼마나 되었는지 판단 할 수 있습니다.
    jti: JWT의 고유 식별자로서, 주로 중복적인 처리를 방지하기 위하여 사용됩니다. 일회용 토큰에 사용하면 유용합니다.
    '''

    '''
    # Header ############################
    {
        "alg": "HS256",
        "typ": "JWT"
    }

    # Payload ###########################
    {
        "sub": "1234567890",
        "name": "John Doe",
        "iat": 1516239022
    }

    # Signature #########################
    HMACSHA256(
      base64UrlEncode(header) + "." +
      base64UrlEncode(payload),
      secret)
    '''

## RetinaFace **프로젝트 사용 라이브러리

## RGB (Blue Green Red)
    openCV는  BGR  사용 -> Red layer 가장 위로 -> 붉게 보임
	cv.imshow('Original', img)
	cv.waitKey(0)
	cv.destroyAllWindows()
    matplot는  RGB 사용 -> Blue layer 가장 위로 -> 푸르게 보임
	plt.imshow((lambda x: Image.fromarray(x))(img))
	plt.show()
## Serializer
    serializer은 쿼리셋과 모델 인스턴스와 같은 복잡한 데이터를
    JSON, XML 또는 다른 컨텐츠의 유형으로 쉽게 변환 할 수 있도록 해줌.
    DJango에서 Client으로 복잡한 데이터(모델 인스턴스 등)를 보내려면
    ‘string’으로 변환해야합니다. 이 변환을 serializer 라고 함.
## Stable Diffusion **프로젝트 사용 라이브러리
## StartGANv2-VC **프로젝트 사용 라이브러리

## StyleGAN2 **프로젝트 사용 라이브러리
## Tacotron2 **프로젝트 사용 라이브러리
## TfidfVectorizer
    TfidfVectorizer는 TF-IDF를 이용해 텍스트 데이터의 특징을 추출하는 것이다.
    TF는 Term Frequency로 특정 단어가 글 안에서 나오는 횟수를 말하며
    IDF는 Inverse Document Frequency로 특정 단어가 여러 글에 얼마나 자주 나오는지
    알려주는 지표의 Inverse(반대) 값이다.
    다른 글에서 지시대명사나 조사가 많이 나오므로 IDF는 값이 반대로 낮은 값을 갖게 된다.
    이것이 TF-IDF의 장점으로 의미가 없는 조사나 지시대명사를 제외한 단어들의 임베딩 값을 얻을 수 있는 것이다.
    TF-IDF는 TF와 IDF를 곱한 값으로 다른 글에서 자주 나오지 않고 해당 문서에 많이 등장할수록
    더 높은 값을 갖게 된다.
## Tortoise **프로젝트 사용 라이브러리
## transform 의 종류
    fit_transform()은 train dataset에서만 사용됩니다
    transform()은 test data에 적용하기 위해 를 사용한다.
## Transformer **프로젝트 사용 라이브러리
## TTS
## Vision Transformer **프로젝트 사용 라이브러리
## Voice Conversion
## Word2Vec
    https://simpling.tistory.com/entry/Embedding-%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0
    위의 두 방법은 단어의 빈도를 기준으로 단어의 벡터화를 하여 특징을 추출하는데 단어 사이의 유사도를 나타내기는 힘들다.
    단어의 특징과 유사도를 나타내 주는 (진정한) embedding은 Word2Vec과 같은 학습을 통한 예측 기반 방법이다.
    이때 분포 가설(Distributed hypothesis)이 등장한다. 분포 가설은 같은 문맥의 단어, 즉 비슷한 위치에 나오는 단어는
    비슷한 의미를 가진다 라는 의미이다. 따라서 어떤 글의 비슷한 위치에 존재하는 단어는 단어 간의 유사도를 높게 측정할 것이다.
    Word2Vec은 CBow와 Skip-gram이 있다. CBow는 어떤 단어를 문맥 안의 주변 단어들을 통해 예측하는 방법이고
    Skip-gram은 반대로 어떤 단어를 가지고 특정 문맥 안의 주변 단어들을 예측하는 과정이다.

## YOLO **프로젝트 사용 라이브러리
## yt-dlp **프로젝트 사용 라이브러리
## Z점수
    표준 점수(標準 點數, Standard score)는 통계학적으로 정규분포를 만들고
    개개의 경우가 표준편차상에 어떤 위치를 차지하는지를 보여주는 차원없는 수치이다.
    표준값, Z값(Z-value), Z 점수(Z score)이라고도 한다.