## 가설(hypothesis)
    p-value (확률값)
    귀무 가설(null hypothesis)이 맞다는 전제 하에,
    표본에서 실제로 관측된 통계치와 '같거나 더 극단적인' 통계치가 관측될 확률이다.
## 객체 검출(Object Detection)과 객체 인식(Object Recognition)
    https://bitcodic.tistory.com/94
    객체 탐지와 객체 인식은 물체 식별을 하는데 유사한 기술이다.
    그러나 이 둘은 실행함에 있어 차이점을 갖는다.
    객체 탐지는 이미지 내의 물체 존재를 찾는 과정이다.
    딥러닝의 경우, 이미지를 식별하는 것 뿐만 아니라 이미지의 위치도 찾는 객체 탐지는 객체 인식의 부분집합이다.
    이는 복수의 물체가 식별되며 같은 이미지 내에 어디에 존재하는지도 가능케한다.
    객체 인식은 정지판을 인식하거나 가로등과 보행자를 구별하는 것을 가능하게 하는 자율주행 차량 내 핵심적인 기술이다.
    이는 또한 바이오이미징과 같은 질병 확인, 산업적 분석 그리고 로봇 비젼과 같은 다양한 애플리케이션에게도 유용하다.
## 검출기
## 결정트리학습(DecisionTree Learning) 에서 불순도를 계산하는 3가지 방법
    https://m.blog.naver.com/samsjang/220978650404
    지니 인덱스
    엔트로피
    분류오류
## 계단 함수(step function)
    동의어로 상수 함수(piecewise-constant function)는
    정의역을 적절한 유한 개의 구간으로 분할하였을 때 각 구간에서 상수 함수가 되는 함수이다.
## 구조화 요소
    침식
    팽창
    열기
    닫기
## 귀무 가설(歸無假說, 영어: null hypothesis, 기호 H0) 또는 영 가설(零假說)
    통계학에서 처음부터 버릴 것을 예상하는 가설이다.
    차이가 없거나 의미있는 차이가 없는 경우의 가설이며
    이것이 맞거나 맞지 않다는 통계학적 증거를 통해 증명하려는 가설이다.
    예를 들어 범죄 사건에서 용의자가 있을 때
    형사는 이 용의자가 범죄를 저질렀다는 추정인 대립가설을 세우게 된다.
    이때 귀무가설은 용의자는 무죄라는 가설이다.
    통계적인 방법으로 가설검정(hypothesis test)을 시도할 때 쓰인다.
    로널드 피셔가 1966년에 정의하였다
## 그라운드 투루쓰 (Ground-Truth)
    https://mac-user-guide.tistory.com/14
    https://towardsdatascience.com/in-ai-the-objective-is-subjective-4614795d179b
     "label"은 정답지라고도 불리듯이 답이 명확하게 정해져 있는 값
     "ground-truth"은 '우리가 정한 정답', '우리의 모델이 우리가 원하는 답으로 예측해주길 바라는 답'
## 기댓값
    확률 변수의 기댓값(expected value)은 각 사건이 벌어졌을 때의 이득과
    그 사건이 벌어질 확률을 곱한 것을 전체 사건에 대해 합한 값이다.
    이것은 어떤 확률적 사건에 대한 평균의 의미로 생각할 수 있다. 이 경우 '모 평균'으로 다룰수있다.
## 나이키스트-새년의 표본화
## 대수의 법칙
## 대수학(algebra)
    변수는 feature 와 target 으로 나뉜다.
    상수는 계수와 편향로 나뉜다.
    따라서 다음과 같은 식의 구조를 같는다.
    target = 계수 * featureValue + 편향
    특성변수 = 독립변수 = 외생변수 = x변수
    목적변수 = 종속변수 = 내생변수 = y변수
    - 계수(係數, coefficient)는 '인자(因子)'라는 뜻으로 쓰인다.
    - 보통 식 앞에 곱해지는 상수를 말한다.
    - 가장 흔한 계수의 개념은 다항식에서 x n 앞에 붙는 수이다.
## 데이터(Data)
    -- Data has a categorical, numeric.
    카테고리(categorical) = 이산형 = norminal + ordinal = 정수형
    숫자형(numeric) = 연속형 = ratio + interval = 실수형
    -- 데이터 분석에는 두 가지의 접근방법이 있다.
    1) 확증적 데이터 분석(CDA: Confirmatory Data Analysis) = 추론통계 = 가설 -> .. -> 특정 사례 예측 = 연역
    2) 탐색적 데이터 분석(EDA: Exploratory Data Analysis) = 기술통계 = 데이터 -> .. -> 모델 = 귀납
## 데이터셋
    https://for-my-wealthy-life.tistory.com/19
    여태까지 공부를 할 때는 train set과 test set 두개로만 데이터를 나누었다.
    다만 이렇게 train, test 두개로만 분리하는 것은 기초적인 수준이고,
    보통 현업에서 모델을 만들 때는 train, test, validation set 세개로 나눈다.
    validation dataset is a sample of data held back from training your model that is used to give an estimate of model skill while tuning model’s hyperparameters.
    The validation dataset is different from the test dataset that is also held back from the training of the model, but is instead used to give an unbiased estimate of the skill of the final tuned model when comparing or selecting between final models.
    There is much confusion in applied machine learning about what a validation dataset is exactly and how it differs from a test dataset.
## 랜덤
## 로지스틱 함수
    https://datascienceschool.net/03%20machine%20learning/10.01%20%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%20%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D.html
    로지스틱 회귀분석 모형에서는 종속변수가 이항분포를 따르고 그 모수 μ가 독립변수 x에 의존한다고 가정한다.
    베르누이 확률분포를 따르는 로지스틱 회귀분석만 고려하기로 한다.
    종속변수 y가 0또는 1인 분류 예측 문제를 풀 때는 x 값을 이용하여 μ(x)를 예측한 후 다음 기준에 따라 y^값을 출력한다.
    y^={10 if μ(x)≥0.5 if μ(x)<0.5
    회귀분석을 할 때는 y^으로 y=1이 될 확률값 μ(x)를 직접 사용한다.
    y^=μ(x)
    로지스틱함수는 음의 무한대부터 양의 무한대까지의 실수값을
    0부터 1사이의 실수값으로 1 대 1 대응시키는 시그모이드함수다.
    보통 시그모이드함수라고 하면 로지스틱함수를 가리킨다.
## 로지스틱회귀
    로지스틱 회귀에는 세 가지 기본 종류가 있습니다.
        이진 로지스틱 회귀: 여기서 범주형 응답에 대해 가능한 결과는 두 가지뿐입니다.
                        위의 예에서와 같이 학생은 합격하거나 불합격합니다.
        다항 로지스틱 회귀: 여기에는 응답 변수에 순서가 없는 3개 이상의 변수가 포함될 수 있습니다.
                        예를 들어 레스토랑에서 식사하는 사람들이 특정 종류의 음식
                        (채식, 고기 또는 완전채식)을 선호하는지 예측하는 것이 있습니다.
        순서 로지스틱 회귀: 다항 회귀와 마찬가지로 3개 이상의 변수가 있을 수 있습니다.
                        그러나 측정에는 순서가 있습니다.
                        예를 들어 1에서 5까지의 척도로 호텔을 평가하는 경우를 들 수 있습니다
## 마르-힐드레스 엣지 검출기
## 마하밴드
## 머신러닝(Machine Learning)
    Machine-Learning has a statistics, deep learning.
    The difference lies in the existence of evidence.
    머신러닝은 통계와 딥러닝의 집합이다.
    머신러닝과 딥러닝의 차이점은 신경망의 유무이다.
## 모델
    데이터를 연료 삼아 작동하는 머신 러닝 모델은 AI의 수학 엔진이자 알고리즘의 표현식으로
    인간보다 빨리 패턴을 찾고 예측을 수행합니다 -엔비디아-
    https://blogs.nvidia.co.kr/2021/08/24/what-is-a-machine-learning-model/
    모델은 선형 또는 로지스틱 회귀와 같은 다양한 유형의 알고리즘을 사용하여 인간의 전문성을 모방하는 방식으로
    데이터에서 패턴을 인식하고 결론을 도출합니다.
    간단히 말해, AI 모델링은 다음 3가지 기본 단계에 따라 의사 결정 프로세스를 생성하는 것입니다.
    https://www.intel.co.kr/content/www/kr/ko/analytics/data-modeling.html
## 분포
    이산확률변수가 따르는 확률분포인 이산확률분포 중,
    대표적인 분포인 베르누이분포(Bernoulli Distribution)와 이항분포(Binomial Distribution)에
    시행의 결과가 성공이면 1의 값을 갖고, 실패이면 0의 값을 갖는 확률변수 X를
    베르누이(Bernoulli) 확률변수라고 하고, 그 분포를 베르누이 분포라고 합니다.
    그리고 이렇게 두 가지의 결과만을 갖는 시행을 베르누이 시행이라고 합니다.
    성공일 확률은 p, 실패일 확률은 1-p = q가 되겠습니다.
    만약 베르누이 시행을 여러 번 하면 어떻게 될까요?
    예를 들어 동전 던지기도, 한번만 하고 끝내는 것이 아니라 5번을 던져서 그중 성공(앞면이라고 가정하죠)이 3번 나올 확률을 계산해볼 수도 있지 않을까요?
    이때 사용하는 확률분포를 이항분포라고 합니다.
    ㅇ 베르누이분포 : X ~ B(1,p)      (1번 만의 베르누이 시행의 성공 확률분포)
    ㅇ 이항분포     : X ~ B(n,p)      (n번 베르누이 시행의 성공 확률분포)
## 베이지안
## 불순도
    다양한 범주들의 개체들이 얼마나 포함되었는가 정도이다.
    여러가지 클래스가 섞여있는 정도이다. 반대로 순수도(purity)는 같은 클래스끼리
    얼마나 많이 포함되어 있는지를 말한다.
    https://computer-science-student.tistory.com/60
## 산술급수 와 기하급수
## 선형회귀(Linear Regression)을 코드로 구현한다.
    핵심키워드: 선형회귀(Linear Regression) 가설(Hypothesis) 비용함수(Cost function)
## 소벨 엣지 연산자
## 손실함수 혹은 비용함수(cost function)는 같은 용어로 통계학, 경제학 등에서 널리 쓰이는 함수로
    머신러닝에서도 손실함수는 예측값과 실제값에 대한 오차를 줄이는 데 사용된다.
    평균제곱오차 또는 음의 로그 우도함수가 있으며, 머신러닝에서도 동일한 손실함수를 사용한다.
## 수학적 형태학
    적중-비적중 변환
    세션화
## 시그모이드 함수
    시그모이드 함수는 S자형 곡선 또는 시그모이드 곡선을 갖는 수학 함수이다.
    시그모이드 함수의 예시로는 첫 번째 그림에 표시된 로지스틱 함수가 있다.
## 알고리즘 성능 측정지표
    https://jxo21.tistory.com/17
    https://datascienceschool.net/03%20machine%20learning/09.04%20%EB%B6%84%EB%A5%98%20%EC%84%B1%EB%8A%A5%ED%8F%89%EA%B0%80.html
    TP (True Positive) 정탐: 양성예측이 맞음 / 오류가 생겨서 보고함
    TN (True Negative) 정담: 음성예측이 맞음 / 오류가 없어서 보고없음
    FP (False Positive) 오탐: 양성예측이 틀림 / 오오류가 없는데 보고함
    FN (False Negative) 미탐: 음성예측이 틀림 / 오오류가 있는데 보고없음
    Precision 정밀도:
    Recall 재현율(=민감도): TP / (TP + FN)
    Accuracy 정확도: TP+ TN / (TP + TN + FP + FN) 맞게 예측한 샘플수의 비율-> 최적화 목적함수로 사용
    Fallout 위양성율: FP / (FP + TN)
## 엔트로피(영어: entropy, 독일어: entropie):https://ko.wikipedia.org/wiki/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC
    열역학적 계의 유용하지 않은 (일로 변환할 수 없는) 에너지의 흐름을 설명할 때 이용되는 상태 함수다.
    통계역학적으로, 주어진 거시적 상태에 대응하는 미시적 상태의 수의 로그로 생각할 수 있다.
    엔트로피는 일반적으로 보존되지 않고, 열역학 제2법칙에 따라 시간에 따라 증가한다.
    독일의 물리학자 루돌프 클라우지우스가 1850년대 초에 도입하였다.
    대개 기호로 라틴 대문자 S를 쓴다.
## 연역과 귀납
    연역은 가정된 전제이다.
    귀납은 개인적 경험이다.
## 영상처리
    HoG(Histogram of Oriented Gradient) 서술자
    blob(Binary Large Object)
    LoG(Laplacian of Gaussian)
    DoG(Difference of Gaussians)
    SIFT(Scale-Invariant Feature Transform)
    동차좌표
    k-d 트리
    RANSAC(RANdom SAmple Consensus)
## 오차 편차 잔차
    오차(Error) : 실제값(y) - 테스트 단계 예측값(y^)
    편차(Deviation) : 예측값평균 - 예측값
    잔차(Residual) :  실제값(y) - 트레인 단계 예측값(y^)
## 우도함수
    가능도 함수로 번역되기도 하고, 영어로는 likelihood function 이라 합니다)
    실현된 데이터(혹은 관찰된 데이터 observed data)로 부터
    특정 통계 모델의 적합성을 확인하는데 주로 이용됩니다.
## 이미지 읽기의 flag는 3가지가 있습니다.
    cv2.IMREAD_COLOR : 이미지 파일을 Color로 읽어들입니다.
                        투명한 부분은 무시되며, Default값입니다.
    cv2.IMREAD_GRAYSCALE : 이미지를 Grayscale로 읽어 들입니다.
                            실제 이미지 처리시 중간단계로 많이 사용합니다.
    cv2.IMREAD_UNCHANGED : 이미지파일을 alpha channel까지 포함하여 읽어 들입니다
    3개의 flag대신에 1, 0, -1을 사용해도 됩니다
    Shape is (512, 512, 3)
    Y축: 512 (앞)
    X축: 512 (뒤)
    3은 RGB 로 되어 있다.
    cv2.waitKey(0) : keyboard입력을 대기하는 함수로
                    0이면 key입력까지 무한대기이며, 특정 시간동안 대기하려면
                    milisecond값을 넣어주면 됩니다.
    cv2.destroyAllWindows() 화면에 나타난 윈도우를 종료합니다.
                            일반적으로 위 3개는 같이 사용됩니다.
## 이진화
    전역 임계치 결정
    적응적 임계치 결정
## 인공지능의 결정적 인물들
    (1)앨런 튜링
    (2)딥러닝의 시초, 월터 피츠
    (3)인공지능의 시조, 존 매카시
    (4)딥러닝의 선구자, 로젠블랫
    (5)환상의 콤비, 사이먼과 뉴얼
    (6)인간은 생각하는 기계, 마빈 민스키
    (7)특이점이 온다, 레이 커즈와일
## 인코딩(encoding)
    문자 -> 숫자
## 잡음(Noise)
    가우시안 잡음(Gaussian Noise)
    점 잡음(Salt-And-Pepper Noise)
## 점확산함수(Point Spread Function)
## 정규화(Normalization)
    feature 의 변환은 표준화(Z-score 정규화)와 정규화가 있다.
    아웃라이어가 있으면 표준화 나머지는 정규화가 낫다.
## 지도학습
    지도 Learning (레이블)
    출력값(=타깃)
    int = 불연속 = 카테고리컬 = 분류 classification
    float = 연속 = 시퀀셜 = 회귀 regression
## 지도학습 분류
    classification / 회귀 regression 로 나뉜다.
    model 은 var 를 잡아내서, class 를 시도한다.
## 집계 함수
    집계 함수란 입력이 여러개의 로우이고, 출력이 하나인 결과인 것을 말합니다.
    테이블의 전체 로우 수를 구하는
    count , 평균( avg ), 총합( sum ), 최대값( max ), 최소값( min ) 등등이
    이런 함수들 입니다.
## 추론과 예측
    https://gentlej90.tistory.com/19
    https://koreapy.tistory.com/1108
    추론과 예측의 차이는 답이 라벨인가, 그라운드트루스 인가이다.
## 최적화함수
    최적화함수(Optimizer Function)는 손실함수의 결과값을 최소화하는 함수이며
    기본적으로 경사하강법(Gradient Descent)을 사용한다.
    최적화함수 또한 컴파일 과정에서 정의하며 다양한 방식들이 있다.
    경사하강법을 개선한 확률적 경사하강법(SGD: Stochastic Gradient Descent)이 있고,
    SGD를 개선한 RMSProp 방식이 있다.
## 타깃
    출력값(=타깃)
     - int = 불연속 = 카테고리컬 = 분류 classification
     - float = 연속 = 시퀀셜 = 회귀 regression
## 통계와 머신러닝 차이
    기술통계 - 추론통계 = 학습(Learning)
## 특성맵
    합성곱에서 리턴되는 함수이다.
## 편향과 편차
    https://opentutorials.org/module/3653/22071
    정답 하나를 맞추기 위해 컴퓨터는 여러 번의 예측값 내놓기를 시도하는데,
    컴퓨터가 내놓은 예측값의 동태를 묘사하는 표현이 '편향' 과 '분산' 입니다.
    예측값들과 정답이 대체로 멀리 떨어져 있으면 결과의 편향(bias)이 높다고 말하고,
    예측값들이 자기들끼리 대체로 멀리 흩어져있으면 결과의 분산(variance)이 높다고 말합니다.
    회귀 문제이든, 분류 문제이든
    첫 번째 그림과 같은 상황을 Underfitting = High Bias
    세 번째 그림과 같은 상황을 Overfitting이라고 합니다. = High Variance
## 패팅(Padding)
    합성곱에서 입력 행렬의 주의를 가상의 원소(0값)으로 채우는 것이다.
    종류는 세임패딩(Same Padding)과 밸리드패딩(Valid Padding)이 있다.
## 프리윗 엣지 연산자
## 필터(Filter)
    OpenCV에서 필터는 다음과 같다
        저역통과 필터
        쌍방 필터
        중간값 필터
    합성곱에서 필터는 뉴런의 행렬구조로 된 집합이다.

## 캐니 엣지 검출기
    1. 스무딩(블러): 가우시안 필터를 이용한 노이즈 제거한다
    2. 그레디언트: 소벨필터를 이용한 그래디언트의 크기(intensity)를 찾는다
    3. Non-maximum suppression을 적용하여 거짓 반응을 제거한다.
    4. 경계선으로써 가능성 있는 픽셀을 골라내기 위해 double threshold 방식을 적용한다.
    5. 앞서 double threshold 방식에서 maxVal을 넘은 부분을 strong edge,
    minVal과 maxVal 사이의 부분을 weak edge로 설정하여 strong edge와 연결되어 있는 weak edge를 edge로 판단하고 그렇지 않는 부분은 제거한다. (Hysteresis thresholding)
## 커널
    합성곱에서 커널은 가중치이다.
## 학습(Learning)
    통계학에서 학습은 추정문제 해결과정(=추론)이다.
    learning 은 target 을 구하는 modeling 이다.
    -- 학습은 두가지 종류가 있다.
    지도학습은 샘플을 사용한다.
    비지도학습은 샘플을 사용하지 않는다.
## 함수형 프로그래밍은 속성을 제거한 프로그래밍 패러다임이다.
    함수형 프로그래밍 언어로 설계된 클로저,스칼라,하스켈 등의 언어가 있고,
    자바, 자바스크립트,코틀린,파이썬 등에도
    최근 버전에 함수형 프로그래밍 문법이 추가 되었다.
    함수형 프로그래밍은 객체 지향형 프로그래밍과는 다른 새로운 방식이다.
    새로운 계산방법을 배우는 것 처럼 사고의 전환을 필요로 하며
    다양한 사고방식으로 프로그래밍을 바라보면 유연한 문제해결이 가능하다.
## 합성곱(convolution)
    합성곱은 함수(=입력)와 함수(=필터)가 커널(가중치)을 곱하여 새로운 함수(=피처맵)를 리턴하는 연산자이다.
## 확률(Probability)
    선험적 통계 = 사전, 수학적 확률, 식 -> 연역법
    경험적 통계 = 사후,  통계적 확률, 식 * "큰수의 법칙" -> 귀납법
    기대값 = 계수 * 변수 + 상수
## 확률변수(確率變數, 영어: random variable)
    확률변수는 확률 공간에서 다른 가측 공간으로 가는 가측 함수이다.
## 확률분포는 함수다
    리턴값에 따라 정수는 PMF, 실수는 PDF 를 사용한다.
    인공지능에서는 Dense 레이어를 사용하므로, 리턴값은 실수로 정의한다.
    확률분포는
    이산 - 확률질량함수 PMF: 이항분포, 다항분포, 이산균등분포, 푸아송분포, 베르누이분포, (초)기하분포
    연속 - 확률밀도함수 PDF: 정규분포(=가우스분포), 연속균등분포, 카이제곱분포, 감마분포
## 행렬 연산(Matrix Operations)
    행렬 표기법 - Matrix Notation
    행렬 덧셈 - Matrix Sum
    스칼라 곱 - Scalar Multiple
    행렬 곱 - Matrix Multiplication
    행렬의 전치 - The transpose of a matrix
## 효과(Effect)
    동시대비효과
    표본화(Sampling)
    양자화(Quantization)
    평활화(Smoothing)
## 휴리스틱 이론
    불충분한 시간이나 데이터로 인해 합리적인 판단이 불가능하거나
    굳이 필요하지 않은 상태에서 빠르게 사용하는 어림짐작
## criterion 은 표준이다. 동의어로는
    standard, normal, norm, average, level 이 있다.
## K-평균 알고리즘
    군집화(Cluster) 문제를 풀기위한 비지도 학습 알고리즘.
    주어진 데이터를 군집갯수(K) 로 그룹화하여 동일한 성질을 가지고
    다른 그룹과 차별화 시키는 것.
    알고리즘의 결과는 중심(centroid) 라고 부르는 K개의 점(dot)으로,
    이름은 각기 다른 그룹의 중심점을 나타낸다.
    k-평균 클러스터링 알고리즘은 클러스터링 방법 중 분할법에 속한다.
    분할법은 주어진 데이터를 여러 파티션 (그룹) 으로 나누는 방법이다.
    예를 들어 n개의 데이터 오브젝트를 입력받았다고 가정하자.
    이 때 분할법은 입력 데이터를 n보다 작거나 같은 k개의 그룹으로 나누는데,
    이 때 각 군집은 클러스터를 형성하게 된다.
    다시 말해, 데이터를 한 개 이상의 데이터 오브젝트로 구성된 k개의 그룹으로 나누는 것이다.
    이 때 그룹을 나누는 과정은 거리 기반의 그룹간
    비유사도 (dissimilarity) 와 같은 비용 함수 (cost function) 을
    최소화하는 방식으로 이루어지며,
    이 과정에서 같은 그룹 내 데이터 오브젝트 끼리의 유사도는 증가하고,
    다른 그룹에 있는 데이터 오브젝트와의 유사도는 감소하게 된다.
    k-평균 알고리즘은 각 그룹의 중심 (centroid)과
    그룹 내의 데이터 오브젝트와의 거리의 제곱합을 비용 함수로 정하고,
    이 함수값을 최소화하는 방향으로 각 데이터 오브젝트의 소속 그룹을
    업데이트 해 줌으로써 클러스터링을 수행하게 된다.
##  Ground-truth
    학습하고자 하는 데이터의 원본 혹은 실제 값을 표현할때 사용됩니다
    https://eair.tistory.com/16
## MSE vs. CCEE
    회귀ML 의 손실함수는 MSE 이다
    분류ML 의 손실함수는 CCEE 이다. 활성화함수로 Softmax 를 사용한다.
## OpenCV
    openCV는  BGR  사용 -> Red layer 가장 위로 -> 붉게 보임
	cv.imshow('Original', img)
	cv.waitKey(0)
	cv.destroyAllWindows()
    matplot는  RGB 사용 -> Blue layer 가장 위로 -> 푸르게 보임
	plt.imshow((lambda x: Image.fromarray(x))(img))
	plt.show()
## RGB (Blue Green Red)
    openCV는  BGR  사용 -> Red layer 가장 위로 -> 붉게 보임
	cv.imshow('Original', img)
	cv.waitKey(0)
	cv.destroyAllWindows()
    matplot는  RGB 사용 -> Blue layer 가장 위로 -> 푸르게 보임
	plt.imshow((lambda x: Image.fromarray(x))(img))
	plt.show()
## transform 의 종류
    fit_transform()은 train dataset에서만 사용됩니다
    transform()은 test data에 적용하기 위해 를 사용한다.
## Z점수
    표준 점수(標準 點數, Standard score)는 통계학적으로 정규분포를 만들고
    개개의 경우가 표준편차상에 어떤 위치를 차지하는지를 보여주는 차원없는 수치이다.
    표준값, Z값(Z-value), Z 점수(Z score)이라고도 한다.